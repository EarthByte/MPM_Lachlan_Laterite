{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49891b0",
   "metadata": {},
   "source": [
    "# Machine learning-based framework for prospectivity mapping of Lateritic Ni-Co Mineralisation using supergene features in the Lachlan Orogen\n",
    "\n",
    "### Ehsan Farahbakhsh, Nathan Wake, R. Ditmar M&uuml;ller\n",
    "\n",
    "*EarthByte Group, School of Geosciences, University of Sydney, NSW 2006, Australia*\n",
    "\n",
    "This notebook enables the user to create a prospectivity map of critical minerals in New South Wales, particularly the Lachlan Orogen. It comprises two main sections; in the first section, the available datasets are visualised, and in the second section, machine learning algorithms are applied to create a prospectivity map.\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "from ipywidgets import interact\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pulearn import BaggingPuClassifier\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import shapely.strtree\n",
    "from skimage import exposure, util\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from tqdm.notebook import tqdm\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, auc, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdeb839",
   "metadata": {},
   "source": [
    "### Co and Ni Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occ = gpd.read_file('./Datasets/Mineral Occurrences_July22/GSNSWDataset/MI_FLAT_TABLE.shp')\n",
    "nsw_bndy = gpd.read_file('./Datasets/Frames/NSW_Boundary/NSW_STATE_POLYGON_shp_GDA94_NoIsland_ACT.shp')\n",
    "lachlan_bndy = gpd.read_file('./Datasets/Frames/Lachlan_Boundary.shp')\n",
    "\n",
    "bounds = nsw_bndy.bounds\n",
    "extent = [bounds.loc[0]['minx'], bounds.loc[0]['maxx'], bounds.loc[0]['miny'], bounds.loc[0]['maxy']]\n",
    "\n",
    "bounds_target = lachlan_bndy.bounds\n",
    "extent_target = [bounds_target.loc[0]['minx'], bounds_target.loc[0]['maxx'], bounds_target.loc[0]['miny'], bounds_target.loc[0]['maxy']]\n",
    "\n",
    "commodity = 'Co'\n",
    "# commodity = 'Ni'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "min_occ_minor = min_occ[min_occ['MINOR_COMM'].str.contains(commodity)==True] # minor commodity\n",
    "min_occ_major = min_occ[min_occ['MAJOR_COMM'].str.contains(commodity)==True] # major commodity\n",
    "min_occ_minor.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "min_occ_major.plot(ax=ax, edgecolor='black', color='orange')\n",
    "nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.set_title(commodity+'-bearing Mineral Occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd8f4fd",
   "metadata": {},
   "source": [
    "### Mineralisation Types of Co and Ni-bearing Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm = gpd.read_file('./Datasets/Mineral Occurrences_July22/GSNSWDataset/cobalt.shp')\n",
    "# comm = gpd.read_file('./Datasets/Mineral Occurrences_July22/GSNSWDataset/nickel.shp')\n",
    "\n",
    "min_types = comm.NSW_CLASS.unique()\n",
    "min_types.sort()\n",
    "\n",
    "print(comm.NSW_CLASS.value_counts())\n",
    "\n",
    "@interact(min_type=min_types)\n",
    "def show_map(min_type):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data = comm[comm.NSW_CLASS==min_type]\n",
    "    data.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title(min_type)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bcd9b9",
   "metadata": {},
   "source": [
    "### Commodities of Ni-Co Laterites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "laterites = gpd.read_file('./Datasets/Mineral Occurrences_July22/GSNSWDataset/ni_co_laterites_lachlan.shp')\n",
    "\n",
    "comms = laterites.MAJOR_COMM.unique()\n",
    "comms.sort()\n",
    "\n",
    "print(laterites.MAJOR_COMM.value_counts())\n",
    "\n",
    "@interact(comm=comms)\n",
    "def show_map(comm):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data = laterites[laterites.MAJOR_COMM==comm]\n",
    "    data.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title(commodity+'-bearing Mineral Occurrences')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc639b4b",
   "metadata": {},
   "source": [
    "### Vector Data Layers\n",
    "\n",
    "#### Intrusion Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac285ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_bndy_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/IntrusionsBndys_FaultedBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/IntrusionsBndys_GeologicalBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/IntrusionsBndys_IntrusiveBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/IntrusionsBndys_UnconformableBndys.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=intrusion_bndy_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Intrusion Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fffb9",
   "metadata": {},
   "source": [
    "#### Metamorphic Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e946bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorphic_bndy_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicBoundaries_Faults.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicBoundaries_GeologicalBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicBoundaries_MetamorphicBndys.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicBoundaries_Unconformities.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=metamorphic_bndy_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Intrusion Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4920b",
   "metadata": {},
   "source": [
    "#### Metamorphic Isograds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4570a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorphic_iso_file = './Datasets/Seamless Geology/GSNSWDataset/MetamorphicIsograds.shp'\n",
    "metamorphic_iso = gpd.read_file(metamorphic_iso_file)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "metamorphic_iso.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.set_title('Metamorphic Isograds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bd038",
   "metadata": {},
   "source": [
    "#### Rock Units, Boundaries, and Fault Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bndy_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_CSP_Geological boundary.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_LAO_Faulted boundary.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_LAO_Geological boundary.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_LAO_Intrusive boundary.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnitBndyFaults/RockUnitBndyFaults_LAO_Unconformable boundary.shp',\n",
    "]\n",
    "\n",
    "@interact(dataset=bndy_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5aaa9",
   "metadata": {},
   "source": [
    "#### Intrusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/Intrusions_Tabberabberan.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=intrusion_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='white', color='blue', linewidth=0.5)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Intrusions')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf76cd",
   "metadata": {},
   "source": [
    "#### Metamorphic Facies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c324394",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fac_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicFacies_Benambran.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicFacies_KanimblanTablelands.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/MetamorphicFacies_Tabberabberan.shp',\n",
    "]\n",
    "\n",
    "@interact(dataset=meta_fac_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='white', color='blue', linewidth=0.5)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Metamorphic Facies')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8d2c2",
   "metadata": {},
   "source": [
    "#### Rock Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77631f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_unit_files = [\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnits_CSP.shp',\n",
    "    './Datasets/Seamless Geology/GSNSWDataset/RockUnits_LAO.shp',\n",
    "]\n",
    "\n",
    "@interact(dataset=rock_unit_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='white', color='blue', linewidth=0.5)\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    laterites.plot(ax=ax, edgecolor='black', color='yellow')\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e83bb",
   "metadata": {},
   "source": [
    "### Raster Data Layers\n",
    "\n",
    "#### Magnetic Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18021703",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnetic_files = [\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_1vd-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_1vd-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_05vd-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_as-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-Bzz-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PGrav-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PGravTHD-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-Phase-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PSusp-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC0m500mRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC1km2kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC2km4kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC4km8kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC8km12kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC12km16kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC16km20kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC24km30kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC36km42kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC42km50kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi_rtp-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi-AWAGS_MAG_2019.tif',\n",
    "    './Datasets/Magnetic/Magmap2019-grid-tmi-Cellsize40m-AWAGS_MAG_2019.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=magnetic_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "        \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e573e5a",
   "metadata": {},
   "source": [
    "#### Gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa84fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity_files = [\n",
    "    './Datasets/Gravity/Gravmap2016-grid-grv_cscba.tif',\n",
    "    './Datasets/Gravity/Gravmap2016-grid-grv_ir.tif',\n",
    "    './Datasets/Gravity/Gravmap2016-grid-grv_scba.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_1vd.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_1vd-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_05vd.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_05vd-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_tilt.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba_tilt-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_cscba-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_1vd.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_1vd-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_05vd.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_05vd-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_tilt.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir_tilt-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_dtgir-IncludesAirborne.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_fa.tif',\n",
    "    './Datasets/Gravity/Gravmap2019-grid-grv_fa-IncludesAirborne.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=gravity_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "        \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb55f8",
   "metadata": {},
   "source": [
    "#### Radiometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b1caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiometric_files = [\n",
    "    './Datasets/Radiometric/Radmap2019-grid-dose_terr-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-dose_terr-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-k_conc-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-k_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-th_conc-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-th_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-thk_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-u_conc-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-u_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-u2th_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-uk_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Datasets/Radiometric/Radmap2019-grid-uth_ratio-AWAGS_RAD_2019.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=radiometric_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "        \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc16b1",
   "metadata": {},
   "source": [
    "#### Remote Sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b13a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_sensing_files = [\n",
    "    './Datasets/Remote Sensing/AlOH_Group_Composition.tif',\n",
    "    './Datasets/Remote Sensing/AlOH_Group_Content.tif',\n",
    "    './Datasets/Remote Sensing/FeOH_Group_Content.tif',\n",
    "    './Datasets/Remote Sensing/Ferric_Oxide_Composition.tif',\n",
    "    './Datasets/Remote Sensing/Ferric_Oxide_Content.tif',\n",
    "    './Datasets/Remote Sensing/Ferrous_Iron_Content_in_MgOH.tif',\n",
    "    './Datasets/Remote Sensing/Ferrous_Iron_Index.tif',\n",
    "    './Datasets/Remote Sensing/Green_Vegetation.tif',\n",
    "    './Datasets/Remote Sensing/Gypsum_Index.tif',\n",
    "    './Datasets/Remote Sensing/Kaolin_Group_Index.tif',\n",
    "    './Datasets/Remote Sensing/MgOH_Group_Composition.tif',\n",
    "    './Datasets/Remote Sensing/MgOH_Group_Content.tif',\n",
    "    './Datasets/Remote Sensing/Opaque_Index.tif',\n",
    "    './Datasets/Remote Sensing/Quartz_Index.tif',\n",
    "    './Datasets/Remote Sensing/Silica_Index.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=remote_sensing_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "        \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d3025",
   "metadata": {},
   "source": [
    "#### Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_files = [\n",
    "    './Datasets/Elevation/Gravmap2019-grid-ausdrape_ellips.tif',\n",
    "    './Datasets/Elevation/Gravmap2019-grid-ausdrape_geoid.tif',\n",
    "    './Datasets/Elevation/Gravmap2019-grid-dem_ellips.tif',\n",
    "    './Datasets/Elevation/Gravmap2019-grid-dem_geoid.tif',\n",
    "    ]\n",
    "\n",
    "@interact(file=elevation_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "        \n",
    "    raster_array = raster.values\n",
    "\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    nsw_bndy.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap='Spectral_r', extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7ac0e",
   "metadata": {},
   "source": [
    "### Extract the Coordinates of Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6715e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the coordinates to a CSV file\n",
    "deposit_coords_file = './Datasets/Outputs/training_data_deposit_coords.csv'\n",
    "\n",
    "if os.path.isfile(deposit_coords_file):\n",
    "    print('The coordinates of deposits already exist.')\n",
    "    deposit_coords = pd.read_csv(deposit_coords_file, index_col=False)\n",
    "    deposit_num = int(deposit_coords.shape[0])\n",
    "    deposit_x = pd.Series.tolist(deposit_coords['X'])\n",
    "    deposit_y = pd.Series.tolist(deposit_coords['Y'])\n",
    "else:\n",
    "    deposit_x = laterites.geometry.x\n",
    "    deposit_y = laterites.geometry.y\n",
    "    deposit_num = laterites.shape[0]\n",
    "    deposit_coords = pd.DataFrame(deposit_x, columns=['X'])\n",
    "    deposit_coords['Y'] = deposit_y\n",
    "    deposit_coords['label'] = 1\n",
    "    deposit_coords.to_csv(deposit_coords_file, index=False)\n",
    "    print(f'The coordinates of deposits have been saved to {deposit_coords_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3937e6",
   "metadata": {},
   "source": [
    "### Prepare the Training Data File\n",
    "\n",
    "#### Vector Data Layers\n",
    "\n",
    "##### Polylines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance from points to linear features\n",
    "def get_dist_lines(xs, ys, lines, column_names):\n",
    "    # construct a spatial tree considering the geometries of all lines\n",
    "    dist_to_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line_tree = shapely.strtree.STRtree(line.geometry)\n",
    "        points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "        dist_to_line = []\n",
    "    \n",
    "        for p in points:\n",
    "            dist_to_line.append(p.distance(line_tree.nearest(p))) # shapely v1.8\n",
    "            # dist_to_line.append(p.distance(line.geometry.iloc[line_tree.nearest(p)])) # shapely v2.0\n",
    "        \n",
    "        dist_to_lines.append(dist_to_line)\n",
    "    \n",
    "    return pd.DataFrame(np.array(dist_to_lines).T, columns=column_names)\n",
    "\n",
    "# concatenate and export the features generated using the functions above\n",
    "def get_vector_data(xs, ys):\n",
    "    lines_files = intrusion_bndy_files + metamorphic_bndy_files + bndy_files\n",
    "    lines_files.append(metamorphic_iso_file)\n",
    "    lines = []\n",
    "    column_names = []\n",
    "    \n",
    "    for file in lines_files:\n",
    "        lines.append(gpd.read_file(file))\n",
    "        column_names.append(os.path.splitext(os.path.basename(file))[0])\n",
    "    \n",
    "    dist_lines_df = get_dist_lines(xs, ys, lines, column_names)\n",
    "    return dist_lines_df\n",
    "\n",
    "deposit_vector_file = f'./Datasets/Outputs/training_data_deposit_vector.csv'\n",
    "\n",
    "if os.path.isfile(deposit_vector_file):\n",
    "    print('The vector dataset (deposits) already exists.')\n",
    "    deposit_vector_data = pd.read_csv(deposit_vector_file, index_col=False)\n",
    "else:\n",
    "    deposit_vector_data = get_vector_data(deposit_x, deposit_y)\n",
    "    deposit_vector_data = deposit_vector_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_vector_data.to_csv(deposit_vector_file, index=False)\n",
    "    print(f'The vector dataset (deposits) has been saved to {deposit_vector_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f20108",
   "metadata": {},
   "source": [
    "##### Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract categorical data\n",
    "def get_cat_data(xs, ys):\n",
    "    polygons_files = intrusion_files + meta_fac_files + rock_unit_files\n",
    "    \n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    cat_code =  pd.DataFrame()\n",
    "    \n",
    "    for file in tqdm(polygons_files):\n",
    "        column_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        polygon = gpd.read_file(file)\n",
    "        temp = []\n",
    "        for point in points:\n",
    "            temp_val = 'Null'\n",
    "            for index, row in polygon.iterrows():\n",
    "                if point.within(row.geometry):\n",
    "                    if column_name.startswith('Intrusions') or column_name.startswith('RockUnits'):\n",
    "                        # temp_val = row.Unit_Name\n",
    "                        temp_val = row.Dominant_L\n",
    "                        break\n",
    "                    elif column_name.startswith('MetamorphicFacies'):\n",
    "                        temp_val = row.MetFacies\n",
    "                        break\n",
    "            \n",
    "            temp.append(temp_val)\n",
    "            \n",
    "        cat_code[column_name] =  temp\n",
    "        \n",
    "    return cat_code\n",
    "\n",
    "# export the features generated using the functions above\n",
    "deposit_cat_file = f'./Datasets/Outputs/training_data_deposit_categorical.csv'\n",
    "\n",
    "if os.path.isfile(deposit_cat_file):\n",
    "    print('The categorical dataset (deposits) already exists.')\n",
    "    deposit_cat_data = pd.read_csv(deposit_cat_file, index_col=False)\n",
    "else:\n",
    "    deposit_cat_data = get_cat_data(deposit_x, deposit_y)\n",
    "    deposit_cat_data = deposit_cat_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_cat_data.to_csv(deposit_cat_file, index=False)\n",
    "    print(f'The categorical dataset (deposits) has been saved to {deposit_cat_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8648f",
   "metadata": {},
   "source": [
    "#### Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd43f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and standard deviation in a buffer zone (circle) surrounding each target point\n",
    "def get_mean_std(xs, ys, raster_np, bounds, radius):\n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    means = []\n",
    "    stds = []\n",
    "    \n",
    "    for point in points:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        xx = []\n",
    "        yy = []\n",
    "        \n",
    "        if x < bounds[0] or x > bounds[2] or y < bounds[1] or y > bounds[3]:\n",
    "            means.append(np.nan)\n",
    "            stds.append(np.nan)\n",
    "        else:\n",
    "            x_origin = math.floor((x-bounds[0])/(bounds[2]-bounds[0])*(raster_np.shape[1]-1))\n",
    "            y_origin = math.floor((y-bounds[1])/(bounds[3]-bounds[1])*(raster_np.shape[0]-1))\n",
    "\n",
    "            radius_= math.ceil(radius*raster_np.shape[1]/(bounds[2]-bounds[0]))\n",
    "            points_circle = []\n",
    "        \n",
    "            for xr in range(-radius_, radius_+1):\n",
    "                Y = int((radius_*radius_-xr*xr)**0.5)\n",
    "                for yr in range(-Y, Y+1):\n",
    "                    xc = xr + x_origin\n",
    "                    yc = yr + y_origin\n",
    "                    if xc >= 0 and xc <= raster_np.shape[1]-1 and yc >= 0 and yc <= raster_np.shape[0]-1:\n",
    "                        points_circle.append((xc, yc))\n",
    "            \n",
    "            for p in points_circle:\n",
    "                xx.append(p[0])\n",
    "                yy.append(raster_np.shape[0]-1-p[1])\n",
    "        \n",
    "            means.append(np.nanmean(raster_np[yy, xx]))\n",
    "            stds.append(np.nanstd(raster_np[yy, xx]))\n",
    "            \n",
    "    return means, stds\n",
    "\n",
    "# calculate dissimilarity and correlation in a window (square) surrounding each target point\n",
    "def get_diss_corr(xs, ys, raster_np, bounds, patch_size):\n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    dissimilarity = []\n",
    "    correlation = []\n",
    "    \n",
    "    for point in points:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        \n",
    "        left = max(x-patch_size/2, bounds[0])\n",
    "        right = min(x+patch_size/2, bounds[2])\n",
    "        top = min(y+patch_size/2, bounds[3])\n",
    "        bottom = max(y-patch_size/2, bounds[1])\n",
    "        \n",
    "        left_idx = math.floor((left-bounds[0])/(bounds[2]-bounds[0])*(raster_np.shape[1]-1))\n",
    "        right_idx = math.floor((right-bounds[0])/(bounds[2]-bounds[0])*(raster_np.shape[1]-1))\n",
    "        bottom_idx = math.floor((bottom-bounds[1])/(bounds[3]-bounds[1])*(raster_np.shape[0]-1))\n",
    "        top_idx = math.floor((top-bounds[1])/(bounds[3]-bounds[1])*(raster_np.shape[0]-1))\n",
    "\n",
    "        xs = np.arange(left_idx, right_idx+1)\n",
    "        ys = np.arange(raster_np.shape[0]-1-top_idx, raster_np.shape[0]-1-bottom_idx+1)\n",
    "        xm, ym = np.meshgrid(xs, ys)\n",
    "        \n",
    "        if np.isnan(raster_np[ym, xm]).all():\n",
    "            dissimilarity.append(np.nan)\n",
    "            correlation.append(np.nan)\n",
    "        else:\n",
    "            raster_scaled = exposure.rescale_intensity(raster_np[ym, xm], in_range=(np.nanmin(raster_np[ym, xm]), np.nanmax(raster_np[ym, xm])), out_range=(0, 1))\n",
    "            raster_ubyte = util.img_as_ubyte(raster_scaled)\n",
    "            glcm = graycomatrix(raster_ubyte, distances=[5], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "            dissimilarity.append(graycoprops(glcm, 'dissimilarity')[0, 0])\n",
    "            correlation.append(graycoprops(glcm, 'correlation')[0, 0])\n",
    "    \n",
    "    return dissimilarity, correlation\n",
    "\n",
    "# concatenate and export the features generated from the functions above\n",
    "def get_grid_data(xs, ys, grid_filenames):\n",
    "    grid_features = []\n",
    "    grid_column_names = []\n",
    "\n",
    "    for grid in tqdm(grid_filenames):\n",
    "        prefix = os.path.splitext(os.path.basename(grid))[0]\n",
    "        grid_column_names.append(prefix+'_mean')\n",
    "        grid_column_names.append(prefix+'_std')\n",
    "        grid_column_names.append(prefix+'_dissimilarity')\n",
    "        grid_column_names.append(prefix+'_correlation')\n",
    "\n",
    "        raster = rxr.open_rasterio(grid, masked=True).squeeze()\n",
    "        \n",
    "        bounds = (raster.rio.bounds())\n",
    "        raster_np = raster.values\n",
    "        \n",
    "        means, stds = get_mean_std(xs, ys, raster_np, bounds, 0.1) # search radius = 0.1\n",
    "        grid_features.append(means)\n",
    "        grid_features.append(stds)\n",
    "\n",
    "        dissimilarity,  correlation = get_diss_corr(xs, ys, raster_np, bounds, 0.2) # side of square = 0.2\n",
    "        grid_features.append(dissimilarity)\n",
    "        grid_features.append(correlation)\n",
    "        \n",
    "        del raster\n",
    "        del raster_np\n",
    "\n",
    "    return pd.DataFrame(np.array(grid_features).T, columns=grid_column_names)\n",
    "\n",
    "grid_filenames = magnetic_files + gravity_files + radiometric_files + remote_sensing_files\n",
    "\n",
    "deposit_grid_file = './Datasets/Outputs/training_data_deposit_grids.csv'\n",
    "\n",
    "if os.path.isfile(deposit_grid_file):\n",
    "    print('The grid dataset (deposits) already exists.')\n",
    "    deposit_grid_data = pd.read_csv(deposit_grid_file, index_col=False)\n",
    "else:\n",
    "    deposit_grid_data = get_grid_data(deposit_x, deposit_y, grid_filenames)\n",
    "    deposit_grid_data = deposit_grid_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_grid_data.to_csv(deposit_grid_file, index=False)\n",
    "    print(f'The grid dataset (deposits) has been saved to {deposit_grid_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean in a buffer zone (circle) surrounding each target point\n",
    "def get_mean(xs, ys, raster_grad, bounds, radius):\n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    means = []\n",
    "    \n",
    "    for point in points:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        xx = []\n",
    "        yy = []\n",
    "        \n",
    "        if x < bounds[0] or x > bounds[2] or y < bounds[1] or y > bounds[3]:\n",
    "            means.append(np.nan)\n",
    "        else:\n",
    "            x_origin = math.floor((x-bounds[0])/(bounds[2]-bounds[0])*(raster_grad.shape[1]-1))\n",
    "            y_origin = math.floor((y-bounds[1])/(bounds[3]-bounds[1])*(raster_grad.shape[0]-1))\n",
    "\n",
    "            radius_ = math.ceil(radius*raster_grad.shape[1]/(bounds[2]-bounds[0]))\n",
    "            points_circle = []\n",
    "        \n",
    "            for xr in range(-radius_, radius_+1):\n",
    "                Y = int((radius_*radius_-xr*xr)**0.5)\n",
    "                for yr in range(-Y, Y+1):\n",
    "                    xc = xr + x_origin\n",
    "                    yc = yr + y_origin\n",
    "                    if xc >= 0 and xc <= raster_grad.shape[1]-1 and yc >= 0 and yc <= raster_grad.shape[0]-1:\n",
    "                        points_circle.append((xc, yc))\n",
    "                    \n",
    "            for p in points_circle:\n",
    "                xx.append(p[0])\n",
    "                yy.append(raster_grad.shape[0]-1-p[1])\n",
    "        \n",
    "            means.append(np.nanmean(raster_grad[yy, xx]))\n",
    "            \n",
    "    return means\n",
    "\n",
    "# calculate gradient\n",
    "def get_gradient_data(xs, ys, elevation_files):\n",
    "    grid_features = []\n",
    "    grid_column_names = []\n",
    "\n",
    "    for grid in tqdm(elevation_files):\n",
    "        prefix = os.path.splitext(os.path.basename(grid))[0]\n",
    "        grid_column_names.append(prefix+'_dx')\n",
    "        grid_column_names.append(prefix+'_dy')\n",
    "\n",
    "        raster = rxr.open_rasterio(grid, masked=True).squeeze()\n",
    "        bounds = (raster.rio.bounds())\n",
    "        raster_np = np.array(raster)\n",
    "        raster_grad_x = np.gradient(raster_np)[1]\n",
    "        raster_grad_y = np.gradient(raster_np)[0]\n",
    "        \n",
    "        means_x = get_mean(xs, ys, raster_grad_x, bounds, 0.5)\n",
    "        means_y = get_mean(xs, ys, raster_grad_y, bounds, 0.5)\n",
    "        grid_features.append(means_x)\n",
    "        grid_features.append(means_y)\n",
    "\n",
    "        del raster\n",
    "        del raster_np\n",
    "\n",
    "    return pd.DataFrame(np.array(grid_features).T, columns=grid_column_names)\n",
    "\n",
    "deposit_elev_file = f'./Datasets/Outputs/training_data_deposit_elevation.csv'\n",
    "\n",
    "if os.path.isfile(deposit_elev_file):\n",
    "    print('The elevation dataset (deposits) already exists.')\n",
    "    deposit_elev_data = pd.read_csv(deposit_elev_file, index_col=False)\n",
    "else:    \n",
    "    deposit_elev_data = get_gradient_data(deposit_x, deposit_y, elevation_files)\n",
    "    deposit_elev_data = deposit_elev_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_elev_data.to_csv(deposit_elev_file, index=False)\n",
    "    print(f'The elevation dataset (deposits) has been saved to {deposit_elev_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2dde1c",
   "metadata": {},
   "source": [
    "#### Create the Training Data File of Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b15492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated using the coordinates of the mineral occurrences (positive samples)\n",
    "deposit_training_data_file = f'./Datasets/Outputs/training_data_deposit.csv'\n",
    "\n",
    "if os.path.isfile(deposit_training_data_file):\n",
    "    print('The training data file (deposits) already exists.')\n",
    "    deposit_training_data = pd.read_csv(deposit_training_data_file, index_col=False)    \n",
    "    deposit_training_data_columns = deposit_training_data.columns.tolist()\n",
    "    deposit_num_data_columns = []\n",
    "    deposit_cat_data_columns = []\n",
    "\n",
    "    for column in deposit_training_data_columns:\n",
    "        if column.startswith('Intrusions_') or column.startswith('MetamorphicFacies') or column.startswith('RockUnits'):\n",
    "            deposit_cat_data_columns.append(column)\n",
    "        else:\n",
    "            deposit_num_data_columns.append(column)\n",
    "    \n",
    "    deposit_num_data_columns.remove('label')\n",
    "    deposit_num_data = deposit_training_data[deposit_num_data_columns]\n",
    "    deposit_cat_data = deposit_training_data[deposit_cat_data_columns]\n",
    "else:\n",
    "    deposit_training_data = pd.concat([\n",
    "        deposit_coords,\n",
    "        deposit_grid_data,\n",
    "        deposit_elev_data,\n",
    "        deposit_vector_data,\n",
    "        deposit_cat_data\n",
    "    ],\n",
    "        axis=1)\n",
    "    \n",
    "    # remove the samples with missing values\n",
    "    deposit_training_data = deposit_training_data.dropna()\n",
    "\n",
    "    # separate numerical and categorical features\n",
    "    deposit_num_data = deposit_training_data[deposit_training_data.columns[3:deposit_training_data.shape[1]-deposit_cat_data.shape[1]]]\n",
    "    deposit_cat_data = deposit_training_data[deposit_training_data.columns[deposit_training_data.shape[1]-deposit_cat_data.shape[1]:deposit_training_data.shape[1]]]\n",
    "\n",
    "    unique_columns_num = []\n",
    "    # romove columns (features) with a unique value from the list of numerical features\n",
    "    for i in range(deposit_num_data.shape[1]):\n",
    "        if len(deposit_num_data.iloc[:, i].round(4).unique()) == 1:\n",
    "            unique_columns_num.append(deposit_num_data.columns[i])\n",
    "\n",
    "    deposit_num_data.drop(unique_columns_num, axis=1, inplace=True)\n",
    "    deposit_cat_data_columns = deposit_cat_data.columns.tolist()\n",
    "    deposit_features = pd.concat([deposit_num_data, deposit_cat_data], axis=1).reset_index(drop=True)\n",
    "    deposit_labels = deposit_training_data[deposit_training_data.columns[2]].reset_index(drop=True)\n",
    "    deposit_training_data = pd.concat([deposit_labels, deposit_features], axis=1).reset_index(drop=True)\n",
    "    deposit_training_data.to_csv(deposit_training_data_file, index=False)\n",
    "    \n",
    "    print(f'The training data file (deposits) has been saved to {deposit_training_data_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248b724",
   "metadata": {},
   "source": [
    "### Random (Unlabelled) Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c90cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and extract the coordinates of a number of random samples within the desired polygon\n",
    "def get_unlab_samples(polygon, num_features):\n",
    "    bounds = polygon.bounds\n",
    "    \n",
    "    rand_x = np.random.uniform(low=extent[0], high=extent[1], size=num_features*10)\n",
    "    rand_y = np.random.uniform(low=extent[2], high=extent[3], size=num_features*10)\n",
    "    \n",
    "    unlab_x = []\n",
    "    unlab_y = []\n",
    "\n",
    "    for x, y in zip(rand_x, rand_y):\n",
    "        if len(unlab_x) == num_features*5:\n",
    "            break\n",
    "        p = Point((x, y))\n",
    "        if p.within(polygon.geometry[0]):\n",
    "            unlab_x.append(x)\n",
    "            unlab_y.append(y)\n",
    "    \n",
    "    return unlab_x, unlab_y\n",
    "\n",
    "num_features = deposit_training_data.shape[1] - 1\n",
    "# export the coordinates of the random samples to a CSV file\n",
    "unlab_coords_file = './Datasets/Outputs/training_data_unlab_coords.csv'\n",
    "\n",
    "if os.path.isfile(unlab_coords_file):\n",
    "    print('The coordinates of unlabelled samples already exist.')\n",
    "    unlab_coords = pd.read_csv(unlab_coords_file, index_col=False)\n",
    "    unlab_x = pd.Series.tolist(unlab_coords['X'])\n",
    "    unlab_y = pd.Series.tolist(unlab_coords['Y'])\n",
    "else:\n",
    "    unlab_x, unlab_y = get_unlab_samples(lachlan_bndy, num_features)\n",
    "    unlab_coords = pd.DataFrame(unlab_x, columns=['X'])\n",
    "    unlab_coords['Y'] = unlab_y\n",
    "    unlab_label = [0]*len(unlab_x)\n",
    "    unlab_coords['label'] = unlab_label\n",
    "    unlab_coords.to_csv(unlab_coords_file, index=False)\n",
    "    print(f'The coordinates of unlabelled samples have been saved to {unlab_coords_file}.')\n",
    "\n",
    "laterites_frame = gpd.read_file('./Datasets/Mineral Occurrences_July22/GSNSWDataset/ni_co_laterites_frame.shp')\n",
    "\n",
    "# plot unlabelled samples\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "ax.scatter(unlab_x, unlab_y, color='blue', edgecolors='black', s=10)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.set_title('Unlabelled Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77b413",
   "metadata": {},
   "source": [
    "#### Create the Training Data File of Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd064601",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlab_vector_file = './Datasets/Outputs/training_data_unlab_vector.csv'\n",
    "\n",
    "if os.path.isfile(unlab_vector_file):\n",
    "    print('The vector dataset (unlabelled samples) already exists.')\n",
    "    unlab_vector_data = pd.read_csv(unlab_vector_file, index_col=False)\n",
    "else:\n",
    "    unlab_vector_data = get_vector_data(unlab_x, unlab_y)\n",
    "    unlab_vector_data = unlab_vector_data[unlab_vector_data.columns.intersection(deposit_vector_data.columns)]\n",
    "    unlab_vector_data.to_csv(unlab_vector_file, index=False)\n",
    "    print(f'The vector dataset (unlabelled samples) has been saved to {unlab_vector_file}.')\n",
    "\n",
    "unlab_cat_file = './Datasets/Outputs/training_data_unlab_categorical.csv'\n",
    "\n",
    "if os.path.isfile(unlab_cat_file):\n",
    "    print('The categorical dataset (unlabelled samples) already exists.')\n",
    "    unlab_cat_data = pd.read_csv(unlab_cat_file, index_col=False)\n",
    "else:\n",
    "    unlab_cat_data = get_cat_data(unlab_x, unlab_y)\n",
    "    unlab_cat_data = unlab_cat_data[unlab_cat_data.columns.intersection(deposit_cat_data.columns)]\n",
    "    unlab_cat_data.to_csv(unlab_cat_file, index=False)\n",
    "    print(f'The categorical dataset (unlabelled samples) has been saved to {unlab_cat_file}.')\n",
    "\n",
    "unlab_grid_file = './Datasets/Outputs/training_data_unlab_grids.csv'\n",
    "\n",
    "if os.path.isfile(unlab_grid_file):\n",
    "    print('The grid dataset (unlabelled samples) already exists.')\n",
    "    unlab_grid_data = pd.read_csv(unlab_grid_file, index_col=False)\n",
    "else:\n",
    "    unlab_grid_data = get_grid_data(unlab_x, unlab_y, grid_filenames)\n",
    "    unlab_grid_data = unlab_grid_data[unlab_grid_data.columns.intersection(deposit_grid_data.columns)]\n",
    "    unlab_grid_data.to_csv(unlab_grid_file, index=False)\n",
    "    print(f'The grid dataset (unlabelled samples) has been saved to {unlab_grid_file}.')\n",
    "    \n",
    "unlab_elev_file = './Datasets/Outputs/training_data_unlab_elevation.csv'\n",
    "\n",
    "if os.path.isfile(unlab_elev_file):\n",
    "    print('The elevation dataset (unlabelled samples) already exists.')\n",
    "    unlab_elev_data = pd.read_csv(unlab_elev_file, index_col=False)\n",
    "else:\n",
    "    unlab_elev_data = get_gradient_data(unlab_x, unlab_y, elevation_files)\n",
    "    unlab_elev_data = unlab_elev_data[unlab_elev_data.columns.intersection(deposit_elev_data.columns)]\n",
    "    unlab_elev_data.to_csv(unlab_elev_file, index=False)\n",
    "    print(f'The elevation dataset (unlabelled samples) has been saved to {unlab_elev_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated in the previous cell\n",
    "unlab_training_data_file = './Datasets/Outputs/training_data_unlab.csv'\n",
    "\n",
    "if os.path.isfile(unlab_training_data_file):\n",
    "    print('The training data file (unlabelled samples) already exists.')\n",
    "    unlab_training_data = pd.read_csv(unlab_training_data_file, index_col=False)\n",
    "else:\n",
    "    unlab_training_data = pd.concat([\n",
    "        unlab_coords,\n",
    "        unlab_grid_data,\n",
    "        unlab_elev_data,\n",
    "        unlab_vector_data,\n",
    "        unlab_cat_data],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # remove missing values\n",
    "    unlab_training_data = unlab_training_data.dropna()\n",
    "    unlab_training_data = unlab_training_data[unlab_training_data.columns.intersection(deposit_training_data.columns)]\n",
    "    unlab_training_data.to_csv(unlab_training_data_file, index=False)\n",
    "    \n",
    "    print(f'The training data file (unlabelled samples) has been saved to {unlab_training_data_file}.')\n",
    "\n",
    "Xy_train_original_df_file = './Datasets/Outputs/Xy_train_original.csv'\n",
    "\n",
    "if os.path.isfile(Xy_train_original_df_file):\n",
    "    Xy_train_original_df = pd.read_csv(Xy_train_original_df_file, index_col=False)\n",
    "    print('Features file already exists!')\n",
    "    \n",
    "    with open('./Datasets/Outputs/st_scaler.pkl', 'rb') as f:\n",
    "        st_scaler = pickle.load(f)\n",
    "        \n",
    "    with open('./Datasets/Outputs/encoder.pkl', 'rb') as f:\n",
    "        enc = pickle.load(f)\n",
    "else:\n",
    "    deposit_labels = deposit_training_data['label']\n",
    "    unlab_labels = unlab_training_data['label']\n",
    "    labels = pd.concat([deposit_labels, unlab_labels]).reset_index(drop=True)    \n",
    "    training_data_original = pd.concat([deposit_training_data, unlab_training_data]).reset_index(drop=True)\n",
    "    \n",
    "    # separate numerical and categorical features\n",
    "    training_data_num = training_data_original[deposit_num_data.columns]\n",
    "    training_data_cat = training_data_original[deposit_cat_data.columns]\n",
    "\n",
    "    features_name_desired_df = pd.read_csv('./Datasets/Outputs/features_name_supergene.csv')\n",
    "    \n",
    "    training_data_num = training_data_original[features_name_desired_df['Numerical'].dropna().tolist()]\n",
    "\n",
    "    # drop highly correlated features\n",
    "    # create a correlation matrix\n",
    "    corr_matrix = training_data_num.corr(method='spearman').abs()\n",
    "    # select the upper triangle of the correlation matrix\n",
    "    corr_upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    # find features with the correlation greater than 0.7\n",
    "    corr_drop = [column for column in corr_upper.columns if any(corr_upper[column] > 0.7)]\n",
    "    print('List of the features removed due to high correlation with other features:', corr_drop)\n",
    "    # drop features\n",
    "    training_data_num_purged = training_data_num.drop(corr_drop, axis=1)\n",
    "    \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(training_data_cat)\n",
    "    training_data_cat_encoded = enc.transform(training_data_cat).toarray()\n",
    "    training_data_cat_columns = enc.get_feature_names(training_data_cat.columns).tolist()\n",
    "    training_data_cat_encoded = pd.DataFrame(training_data_cat_encoded, columns=training_data_cat_columns)\n",
    "    training_data_cat_encoded = training_data_cat_encoded[features_name_desired_df['Categorical'].dropna().tolist()]\n",
    "    \n",
    "    features_labels_encoded = pd.concat([training_data_num_purged, training_data_cat_encoded, labels], axis=1).reset_index(drop=True)\n",
    "    features_labels_list = features_labels_encoded.columns.tolist()\n",
    "    features_list = features_labels_list.copy()\n",
    "    features_list.remove('label')\n",
    "\n",
    "    deposit_data = features_labels_encoded[features_labels_encoded['label']==1]\n",
    "    unlab_data = features_labels_encoded[features_labels_encoded['label']==0]\n",
    "\n",
    "    deposit_features = deposit_data[deposit_data.columns[:-1]]\n",
    "    unlab_features = unlab_data[unlab_data.columns[:-1]]\n",
    "\n",
    "    deposit_labels = deposit_data[deposit_data.columns[-1]]\n",
    "    unlab_labels = unlab_data[unlab_data.columns[-1]]\n",
    "\n",
    "    # train test\n",
    "    # split positive samples into training and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(deposit_features, deposit_labels, train_size=0.75, random_state=42)\n",
    "    X_train = np.vstack((X_train, unlab_features))\n",
    "    y_train = np.vstack((y_train.values.reshape(-1, 1), unlab_labels.values.reshape(-1, 1)))\n",
    "    Xy_train_original = np.hstack((X_train, y_train))\n",
    "    Xy_train_original_df = pd.DataFrame(Xy_train_original, columns=features_labels_list)\n",
    "    Xy_train_original_df.to_csv('./Datasets/Outputs/Xy_train_original.csv', index=False)\n",
    "    X_test_df = pd.DataFrame(X_test, columns=features_list).reset_index(drop=True)\n",
    "    y_test_df = pd.DataFrame(y_test, columns=['label']).reset_index(drop=True)\n",
    "    \n",
    "    X_train_num = Xy_train_original_df[training_data_num_purged.columns]\n",
    "    \n",
    "    st_scaler = StandardScaler()\n",
    "    X_train_num = st_scaler.fit_transform(X_train_num)\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=training_data_num_purged.columns)\n",
    "    \n",
    "    X_test_num = X_test_df[training_data_num_purged.columns]\n",
    "    X_test_num = st_scaler.transform(X_test_num)\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=training_data_num_purged.columns)\n",
    "    \n",
    "    Xy_train = pd.concat([X_train_num, Xy_train_original_df[features_name_desired_df['Categorical'].dropna().tolist()], Xy_train_original_df['label']], axis=1).reset_index(drop=True)\n",
    "    Xy_test = pd.concat([X_test_num, X_test_df[features_name_desired_df['Categorical'].dropna().tolist()], y_test_df], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    Xy_train.to_csv('./Datasets/Outputs/Xy_train.csv', index=False)\n",
    "    Xy_test.to_csv('./Datasets/Outputs/Xy_test.csv', index=False)\n",
    "    \n",
    "    # save the standard scaler model\n",
    "    with open('./Datasets/Outputs/st_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(st_scaler, f)\n",
    "        \n",
    "    # save the encoder model\n",
    "    with open('./Datasets/Outputs/encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(enc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f43089",
   "metadata": {},
   "source": [
    "### Create the Predictive Model\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "##### Plot the Bayesian optimization progress for the training data file with the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e49b2-2435-4a26-b8bd-bcceda7e7f27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_file = './Datasets/Outputs/model.pkl'\n",
    "\n",
    "if os.path.isfile(model_file):\n",
    "    print('A model file exists. Attempting to load...')\n",
    "    try:\n",
    "        with open(model_file, 'rb') as f:\n",
    "            bc_best = pickle.load(f)\n",
    "        print('Model loaded successfully.')\n",
    "        Xy_train_df = pd.read_csv('./Datasets/Outputs/Xy_train_.csv', index_col=False)\n",
    "    except (ModuleNotFoundError, ImportError) as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        print(\"This may be due to a version mismatch. Proceeding to train a new model.\")\n",
    "        bc_best = None\n",
    "else:\n",
    "    print('No existing model found. Proceeding to train a new model.')\n",
    "    bc_best = None\n",
    "\n",
    "if bc_best is None:\n",
    "    # Random Forest model structure\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "    bc = BaggingPuClassifier(rf, n_jobs=-1, random_state=42)\n",
    "\n",
    "    n_files = 10\n",
    "    n_fold = 10\n",
    "    acc_best = 0\n",
    "    importances = []\n",
    "    best_iteration_accuracies = None\n",
    "\n",
    "    for i in range(n_files):\n",
    "        print('--------------------')\n",
    "        print(f'Training Data File {i+1}')\n",
    "        print('--------------------')\n",
    "    \n",
    "        smote_gan_file = f'./Datasets/Outputs/smote_gan_{i+1}.csv'\n",
    "        smote_gan = pd.read_csv(smote_gan_file, index_col=False)\n",
    "        features = smote_gan[smote_gan.columns[:-1]]\n",
    "        labels = smote_gan[smote_gan.columns[-1]]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.75, random_state=42)\n",
    "        Xy_train = np.hstack((X_train, y_train.values.reshape(-1, 1)))\n",
    "        Xy_train_df = pd.DataFrame(Xy_train, columns=smote_gan.columns)\n",
    "        y_train.replace(2, 1, inplace=True)\n",
    "        y_test.replace(2, 1, inplace=True)\n",
    "\n",
    "        search_space = {\n",
    "        'base_estimator__bootstrap': Categorical([True, False]),\n",
    "        'base_estimator__max_depth': Integer(5, 20),\n",
    "        'base_estimator__max_features': Categorical([None, 'sqrt','log2']), \n",
    "        'base_estimator__min_samples_leaf': Integer(2, 20),\n",
    "        'base_estimator__min_samples_split': Integer(2, 30),\n",
    "        'base_estimator__n_estimators': Integer(10, 200),\n",
    "        'max_samples': Integer(int(0.5*(len(y_train)-sum(y_train))), int(0.9*(len(y_train)-sum(y_train))))\n",
    "        }\n",
    "\n",
    "        bc_bayes_search = BayesSearchCV(bc, search_space, n_iter=50,\n",
    "                                        scoring='accuracy',\n",
    "                                        n_jobs=-1, cv=n_fold, \n",
    "                                        verbose=1, random_state=42)\n",
    "        bc_bayes_search.fit(X_train, y_train)\n",
    "\n",
    "        # Extract the optimization results\n",
    "        optimization_results = bc_bayes_search.cv_results_['mean_test_score']\n",
    "\n",
    "        X_pred = bc_bayes_search.best_estimator_.predict(X_test)\n",
    "        X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "\n",
    "        if X_pred_acc > acc_best:\n",
    "            acc_best = X_pred_acc\n",
    "            X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "            bc_best = bc_bayes_search.best_estimator_\n",
    "            bc_best_acc = bc_bayes_search.best_score_\n",
    "            bc_best_acc_test = X_pred_acc.copy()\n",
    "            bc_best_pre_test = precision_score(y_test, X_pred)\n",
    "            bc_best_rec_test = recall_score(y_test, X_pred)\n",
    "            bc_best_f1_test = f1_score(y_test, X_pred)\n",
    "            Xy_train_df.to_csv('./Datasets/Outputs/Xy_train_.csv', index=False)\n",
    "            best_iteration_accuracies = optimization_results  # Store accuracies for the best iteration\n",
    "            print('\\nFile number with the highest accuracy:', i+1)\n",
    "            print('Accuracy:', acc_best)\n",
    "\n",
    "        estimators = bc_bayes_search.best_estimator_.estimators_\n",
    "        importances.append([estimators[j].feature_importances_.reshape(-1, 1) for j in range(len(estimators))])\n",
    "    \n",
    "    print('\\nThe highest accuracy during cross validation:', bc_best_acc)\n",
    "    print('Accuracy:', bc_best_acc_test)\n",
    "    print('Precision:', bc_best_pre_test)\n",
    "    print('Recall:', bc_best_rec_test)\n",
    "    print('F1-Score:', bc_best_f1_test)\n",
    "    \n",
    "    # save the model\n",
    "    with open(model_file, 'wb') as f:\n",
    "        pickle.dump(bc_best, f)\n",
    "    \n",
    "    importances_ = []\n",
    "    for i in range(len(importances)):\n",
    "        for j in range(n_fold):\n",
    "            importances_.append(importances[i][j])\n",
    "\n",
    "    importances = np.hstack(importances_)\n",
    "\n",
    "    # Plot the Bayesian optimization progress for the best iteration\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, len(best_iteration_accuracies) + 1), best_iteration_accuracies, marker='o', color='black', markerfacecolor='red')\n",
    "    plt.xlim(0, 51)\n",
    "    plt.xlabel('Bayesian Optimization Iteration')\n",
    "    plt.ylabel('Mean Test Accuracy')\n",
    "    plt.title('Bayesian Optimization Progress')\n",
    "    plt.grid(True, linestyle=':')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./Datasets/Outputs/bayesian_optimization_progress.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Using the loaded model. No new training performed.\")\n",
    "    print(\"To retrain the model, delete or rename the existing model file and run the script again.\")\n",
    "\n",
    "print(\"Script execution completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_estimated = []\n",
    "for pair in bc_best.oob_decision_function_:\n",
    "    if np.isnan(pair[0]) or pair[0] < pair[1]:\n",
    "        labels_estimated.append(2)\n",
    "    else:\n",
    "        labels_estimated.append(0)\n",
    "        \n",
    "print('Number of positive samples', labels_estimated.count(2))\n",
    "print('Number of negative samples', labels_estimated.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test = pd.read_csv('./Datasets/Outputs/Xy_test.csv', index_col=False)\n",
    "X_test = Xy_test[Xy_test.columns[:-1]]\n",
    "y_test = Xy_test[Xy_test.columns[-1]]\n",
    "X_pred = bc_best.predict(X_test)\n",
    "X_pred_acc = accuracy_score(y_test, X_pred)\n",
    "print('Accuracy (Real Positive Samples):', X_pred_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b49eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_gan_file = f'./Datasets/Outputs/smote_gan_1.csv' # smote-gan file provided the highest accuracy\n",
    "smote_gan = pd.read_csv(smote_gan_file, index_col=False)\n",
    "\n",
    "features = smote_gan[smote_gan.columns[:-1]]\n",
    "labels = smote_gan[smote_gan.columns[-1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.75, random_state=42)\n",
    "y_train.replace(2, 1, inplace=True)\n",
    "y_test.replace(2, 1, inplace=True)\n",
    "\n",
    "def roc_plot(y_test, z_test, n_classes, labels_name, average='macro'):\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "\n",
    "    y_test_dummies = pd.get_dummies(y_test).values\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], z_test[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # roc for each class\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver Operating Characteristic')\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        ax.plot(fpr[i], tpr[i], label='{}, AUC = {}'.format(labels_name[i], '{0:.4f}'.format(roc_auc[i])))\n",
    "    \n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(alpha=0.5)\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "    print('ROC AUC score:', roc_auc_score(y_test_dummies, z_test, average=average))\n",
    "\n",
    "# predict for the test dataset\n",
    "z_test = bc_best.predict_proba(X_test)\n",
    "z_test_class = bc_best.predict(X_test)\n",
    "# calculate and display error metrics\n",
    "cMatrix = confusion_matrix(y_test, z_test_class)\n",
    "aScore = accuracy_score(y_test, z_test_class)\n",
    "pScore = precision_score(y_test, z_test_class, average='macro')\n",
    "rScore = recall_score(y_test, z_test_class, average='macro')\n",
    "fscore = f1_score(y_test, z_test_class, average='macro')\n",
    "\n",
    "print('Confusion matrix:\\n', cMatrix)\n",
    "print('\\nAccuracy: %.3f, Precision: %.3f, Recall: %.3f, F1-Score: %.3f' % (aScore, pScore, rScore, fscore))\n",
    "\n",
    "labels_name = ['Background', 'Anomaly']\n",
    "roc_plot(y_test, z_test, 2, labels_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6220f",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_file = './Datasets/Outputs/features_importance.csv'\n",
    "\n",
    "if os.path.isfile(features_importance_file):\n",
    "    features_importance = pd.read_csv(features_importance_file, index_col=False).to_numpy().tolist()\n",
    "else:\n",
    "    output_features = Xy_train_original_df.columns.tolist()\n",
    "    output_features.remove('label')\n",
    "    \n",
    "    importances_mean = importances.mean(axis=1)\n",
    "    importances_var = importances.var(axis=1)\n",
    "\n",
    "    features_importance = [(feature, round(importance, 5)) for feature, importance in zip(output_features, importances_mean)]\n",
    "    features_importance = sorted(features_importance, key=lambda x:x[1], reverse=True)\n",
    "    features_importance_df = pd.DataFrame(features_importance, columns=['Feature', 'Importance'])\n",
    "    features_importance_df['Variance'] = importances_var\n",
    "    features_importance_df.to_csv(features_importance_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e63d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in features_importance]\n",
    "# cumulative importance\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "x_values = list(range(len(features_importance)))\n",
    "x_values = [x+1 for x in x_values]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax1 = ax2.twinx()\n",
    "\n",
    "ax2.bar(x_values, sorted_importances, edgecolor='gray', facecolor='LightSalmon', width=1, alpha=0.5)\n",
    "ax1.plot(x_values, cumulative_importances, 'k--')\n",
    "\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlim(0.5, len(cumulative_importances)+0.5)\n",
    "\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "ax2.set_ylabel('Feature Importance')\n",
    "ax1.set_ylabel('Cumulative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b776f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print significant features above some threshold\n",
    "features_importance_ = features_importance[:20]\n",
    "features_importance_.sort(key=lambda x:x[1])\n",
    "ft_imps = [x[1] for x in features_importance_]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('whitesmoke')\n",
    "bar = ax.barh(range(len(ft_imps)), ft_imps)\n",
    "\n",
    "def gradientbars(bars, data):\n",
    "    ax = bars[0].axes\n",
    "    lim = ax.get_xlim()+ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        bar.set_zorder(1)\n",
    "        bar.set_facecolor('none')\n",
    "        bar.set_edgecolor('black')\n",
    "        x, y = bar.get_xy()\n",
    "        w, h = bar.get_width(), bar.get_height()\n",
    "        cmap = plt.get_cmap('winter')\n",
    "        grad = np.atleast_2d(np.linspace(0, 1*w/max(data), 256))\n",
    "        ax.imshow(grad, extent=[x, x+w, y, y+h], aspect='auto', zorder=0, norm=mpl.colors.NoNorm(vmin=0, vmax=1), cmap=cmap, alpha=0.8)\n",
    "        manual_labels = [x[0] for x in features_importance_]\n",
    "        ax.set_yticks(np.arange(0, len(data), 1).tolist())\n",
    "        ax.set_yticklabels(manual_labels, minor=False)\n",
    "    ax.axis(lim)\n",
    "\n",
    "gradientbars(bar, ft_imps)\n",
    "plt.gca().yaxis.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebdbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_df = pd.read_csv('./Datasets/Outputs/Xy_train_.csv', index_col=False)\n",
    "\n",
    "for i in range(len(labels_estimated)):\n",
    "    if Xy_train_df['label'][i] == 1:\n",
    "        labels_estimated[i] = 1\n",
    "\n",
    "Xy_train_postpul = Xy_train_df.copy()\n",
    "Xy_train_postpul['label'] = labels_estimated\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_facecolor('whitesmoke')\n",
    "    ax2 = ax1.twiny()\n",
    "    \n",
    "    ax1.hist(Xy_train_original_df[feature], bins=25, alpha=0.0)\n",
    "\n",
    "    h1 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==0][feature], bins=25, color='0.8', label='Negative')\n",
    "    h2 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==2][feature], bins=25, color='LightSalmon', label='Synthetic Positive', alpha=0.8)\n",
    "    h3 = ax2.hist(Xy_train_postpul.loc[Xy_train_postpul['label']==1][feature], bins=25, color='DarkSeaGreen', label='Positive', alpha=0.4)\n",
    "\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7eb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_postpul_pivot = Xy_train_postpul.pivot(columns=['label'])\n",
    "Xy_train_original_df_pivot = Xy_train_original_df.pivot(columns=['label'])\n",
    "\n",
    "nb_groups1 = Xy_train_postpul['label'].nunique()\n",
    "nb_groups2 = Xy_train_original_df['label'].nunique()\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_postpul.columns):\n",
    "    bplot1 = [Xy_train_postpul_pivot[feature][var].dropna() for var in Xy_train_postpul_pivot[feature]]\n",
    "    bplot2 = [Xy_train_original_df_pivot[feature][var].dropna() for var in Xy_train_original_df_pivot[feature]]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 10))\n",
    "    box_param1 = dict(whis=(5, 95), widths=0.2, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='black', fillstyle=None),\n",
    "                      medianprops=dict(color='black'), boxprops=dict(facecolor='tab:blue'))\n",
    "    box_param2 = dict(whis=(5, 95), widths=0, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='none', fillstyle=None),\n",
    "                      medianprops=dict(color='none'), whiskerprops=dict(color='none'),\n",
    "                      boxprops=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    ax1.boxplot(bplot1, positions=np.arange(nb_groups1), **box_param1)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.boxplot(bplot2, positions=np.arange(nb_groups2), **box_param2)\n",
    "\n",
    "    # format x ticks\n",
    "    labelsize = 12\n",
    "    ax1.set_xticks(np.arange(nb_groups1))\n",
    "    ax1.set_xticklabels(['Negative', 'Positive', 'Synthetic Positive'])\n",
    "    ax1.tick_params(axis='x', labelsize=labelsize)\n",
    "\n",
    "    # format y ticks\n",
    "    yticks_fmt = dict(axis='y', labelsize=labelsize)\n",
    "\n",
    "    # format axes labels\n",
    "    label_fmt = dict(size=12, labelpad=15)\n",
    "    ax1.set_xlabel(feature, **label_fmt)\n",
    "    ax1.set_ylabel(feature + '\\n(Standardised)', **label_fmt)\n",
    "    ax2.set_ylabel(feature + '\\n(Actual)', **label_fmt)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cc81b",
   "metadata": {},
   "source": [
    "### Generate Target Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the sampling resolution\n",
    "size_x = 0.05\n",
    "size_y = 0.05\n",
    "\n",
    "range_x = np.arange(extent_target[0], extent_target[1], size_x)\n",
    "range_y = np.arange(extent_target[2], extent_target[3], size_y)\n",
    "\n",
    "num_x = len(range_x)\n",
    "num_y = len(range_y)\n",
    "\n",
    "xs, ys = np.meshgrid(range_x, range_y)\n",
    "\n",
    "target_coords_file = './Datasets/Outputs/target_coords.csv'\n",
    "target_mask_file = './Datasets/Outputs/target_mask.csv'\n",
    "\n",
    "# export the coordinates of the target points and create a mask to keep the points only inside the target polygon boundaries\n",
    "if os.path.isfile(target_coords_file) and os.path.isfile(target_mask_file):\n",
    "    print('The coordinates of target points already exist.')\n",
    "    target_coords = pd.read_csv(target_coords_file, index_col=False)\n",
    "    target_x = target_coords['X']\n",
    "    target_y = target_coords['Y']\n",
    "    target_mask = genfromtxt(target_mask_file, delimiter=',')\n",
    "else:\n",
    "    target_x = []\n",
    "    target_y = []\n",
    "    target_mask = []\n",
    "    for xx, yy in zip(xs.flatten(), ys.flatten()):\n",
    "        p = Point((xx, yy))\n",
    "        if p.within(lachlan_bndy.geometry[0]):\n",
    "            target_x.append(xx)\n",
    "            target_y.append(yy)\n",
    "            target_mask.append(True)\n",
    "        else:\n",
    "            target_mask.append(False)\n",
    "    \n",
    "    target_coords = pd.DataFrame(target_x, columns=['X'])\n",
    "    target_coords['Y'] = target_y\n",
    "    target_coords.to_csv(target_coords_file, index=False)\n",
    "    \n",
    "    mask_x = np.array([xs.flatten()]).T\n",
    "    mask_y = np.array([ys.flatten()]).T\n",
    "    target_mask_ = np.array([target_mask]).T\n",
    "    target_mask = np.hstack((mask_x, mask_y, target_mask_))\n",
    "    np.savetxt(target_mask_file, target_mask, delimiter=',')\n",
    "    print('The coordinates of target points and mask have been saved.')\n",
    "\n",
    "print(f'Number of samples: ', len(target_x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(target_x, target_y, color='black', edgecolors='none', s=2)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.set_title('Target Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20e8fe",
   "metadata": {},
   "source": [
    "#### Extract Values of Features at Target Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vector_file = './Datasets/Outputs/target_vector.csv'\n",
    "\n",
    "if os.path.isfile(target_vector_file):\n",
    "    print('The vector dataset (target points) already exists.')\n",
    "    target_vector_data = pd.read_csv(target_vector_file, index_col=False)\n",
    "else:\n",
    "    target_vector_data = get_vector_data(target_x, target_y)\n",
    "    target_vector_data = target_vector_data[target_vector_data.columns.intersection(deposit_vector_data.columns)]\n",
    "    target_vector_data.to_csv(target_vector_file, index=False)\n",
    "    print(f'The vector dataset (target points) has been saved to {target_vector_file}.')\n",
    "\n",
    "target_cat_file = './Datasets/Outputs/target_categorical.csv'\n",
    "\n",
    "if os.path.isfile(target_cat_file):\n",
    "    print('The categorical dataset (target points) already exists.')\n",
    "    target_cat_data = pd.read_csv(target_cat_file, index_col=False)\n",
    "else:\n",
    "    target_cat_data = get_cat_data(target_x, target_y)\n",
    "    target_cat_data = target_cat_data[target_cat_data.columns.intersection(deposit_cat_data.columns)]\n",
    "    target_cat_data.to_csv(target_cat_file, index=False)\n",
    "    print(f'The categorical dataset (target points) has been saved to {target_cat_file}.')\n",
    "\n",
    "target_grid_file = './Datasets/Outputs/target_grids.csv'\n",
    "\n",
    "if os.path.isfile(target_grid_file):\n",
    "    print('The grid dataset (target points) already exists.')\n",
    "    target_grid_data = pd.read_csv(target_grid_file, index_col=False)\n",
    "else:\n",
    "    target_grid_data = get_grid_data(target_x, target_y, grid_filenames)\n",
    "    target_grid_data = target_grid_data[target_grid_data.columns.intersection(deposit_grid_data.columns)]\n",
    "    target_grid_data.to_csv(target_grid_file, index=False)\n",
    "    print(f'The grid dataset (target points) has been saved to {target_grid_file}.')\n",
    "    \n",
    "target_elev_file = './Datasets/Outputs/target_elevation.csv'\n",
    "\n",
    "if os.path.isfile(target_elev_file):\n",
    "    print('The elevation dataset (target points) already exists.')\n",
    "    target_elev_data = pd.read_csv(target_elev_file, index_col=False)\n",
    "else:\n",
    "    target_elev_data = get_gradient_data(target_x, target_y, elevation_files)\n",
    "    target_elev_data = target_elev_data[target_elev_data.columns.intersection(deposit_elev_data.columns)]\n",
    "    target_elev_data.to_csv(target_elev_file, index=False)\n",
    "    print(f'The elevation dataset (target poitns) has been saved to {target_elev_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated in the previous cell\n",
    "target_data_file = './Datasets/Outputs/target_data.csv'\n",
    "\n",
    "if os.path.isfile(target_data_file):\n",
    "    print('The target data file already exists.')\n",
    "    target_data = pd.read_csv(target_data_file, index_col=False)\n",
    "else:\n",
    "    target_data = pd.concat([target_coords,\n",
    "                             target_grid_data,\n",
    "                             target_elev_data,\n",
    "                             target_vector_data,\n",
    "                             target_cat_data],\n",
    "                            axis=1)\n",
    "    target_data.to_csv(target_data_file, index=False)\n",
    "    print(f'The target data file has been saved to {target_data_file}.')\n",
    "\n",
    "target_features_file = './Datasets/Outputs/target_features.csv'\n",
    "\n",
    "if os.path.isfile(target_features_file):\n",
    "    print('The target features file already exists.')\n",
    "    target_features = pd.read_csv(target_features_file, index_col=False)\n",
    "    target_coords_purged = pd.read_csv('./Datasets/Outputs/target_coords_purged.csv', index_col=False)\n",
    "    target_mask = genfromtxt(target_mask_file, delimiter=',')\n",
    "else:\n",
    "    index_null = target_data[target_data.isnull().any(axis=1)].index.to_list()\n",
    "    \n",
    "    for index in index_null:\n",
    "        for i in range(target_mask.shape[0]):\n",
    "            if target_data['X'][index] == target_mask[i, 0] and target_data['Y'][index] == target_mask[i, 1]:\n",
    "                target_mask[i, 2] = False\n",
    "\n",
    "    np.savetxt(target_mask_file, target_mask, delimiter=',')\n",
    "    \n",
    "    # remove missing values\n",
    "    target_data = target_data.dropna()\n",
    "    target_coords_purged = target_data[['X', 'Y']]\n",
    "    target_coords_purged.to_csv('./Datasets/Outputs/target_coords_purged.csv', index=False)\n",
    "\n",
    "    features_label_list = Xy_train_original_df.columns.tolist()\n",
    "    features_list = features_label_list.copy()\n",
    "    features_list.remove('label')\n",
    "    num_features_list = []\n",
    "    cat_features_list = []\n",
    "\n",
    "    for column in features_list:\n",
    "        if column.startswith('Intrusions_') or column.startswith('MetamorphicFacies') or column.startswith('RockUnits'):\n",
    "            cat_features_list.append(column)\n",
    "        else:\n",
    "            num_features_list.append(column)\n",
    "    \n",
    "    # separate numerical and categorical features\n",
    "    target_data_num = target_data[num_features_list]\n",
    "    target_data_cat = target_data[deposit_cat_data_columns]\n",
    "\n",
    "    st_scaler.fit(target_data_num)\n",
    "    target_features_num = st_scaler.transform(target_data_num)\n",
    "    target_features_cat = enc.transform(target_data_cat).toarray()\n",
    "    target_features_cat_columns = enc.get_feature_names(deposit_cat_data.columns).tolist()\n",
    "    target_features_cat = pd.DataFrame(target_features_cat, columns=target_features_cat_columns)\n",
    "    target_features_cat = target_features_cat[features_name_desired_df['Categorical'].dropna().tolist()]\n",
    "    target_features = np.hstack((target_features_num, target_features_cat))\n",
    "    target_features = pd.DataFrame(target_features, columns=features_list)\n",
    "    target_features.to_csv(target_features_file, index=False)\n",
    "    print(f'The target features file has been saved to {target_features_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72787d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(feature=target_features.columns.tolist())\n",
    "def show_map(feature):\n",
    "    feature_values = []\n",
    "    count = 0\n",
    "\n",
    "    for mask in target_mask[:, 2]:\n",
    "        if mask:\n",
    "            feature_values.append(target_features.iloc[count][feature])\n",
    "            count += 1\n",
    "        else:\n",
    "            feature_values.append(np.nan)\n",
    "\n",
    "    feature_values_2d = np.reshape(feature_values, (num_y, num_x))\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "    lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "\n",
    "    cb = plt.imshow(feature_values_2d, cmap='Spectral_r', origin='lower', interpolation='bilinear', extent=extent_target)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    plt.colorbar(cb, orientation='horizontal', label=feature, cax=cax)\n",
    "    ax.set_title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b0c4e",
   "metadata": {},
   "source": [
    "### Calculate Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92284f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_probs_file = './Datasets/Outputs/target_probs.csv'\n",
    "\n",
    "if os.path.isfile(target_probs_file):\n",
    "    print('The probability file already exists.')\n",
    "    target_probs = pd.read_csv(target_probs_file, index_col=False)\n",
    "else:\n",
    "    probs = bc_best.predict_proba(target_features)\n",
    "    \n",
    "    mm_scaler = MinMaxScaler()\n",
    "    probs_scaled = mm_scaler.fit_transform(probs[:, 1].reshape(-1, 1))\n",
    "    \n",
    "    target_probs = target_coords_purged.reset_index().copy()\n",
    "    target_probs['prob'] = probs_scaled\n",
    "    target_probs.to_csv(target_probs_file, index=False)\n",
    "    print(f'The probability file has been saved to {target_probs_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557397a",
   "metadata": {},
   "source": [
    "#### Plot Prospectivity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae198dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability map\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "cb = plt.scatter(target_probs['X'], target_probs['Y'], 30, c=target_probs['prob'], cmap='Spectral_r')\n",
    "laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "\n",
    "# colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "ax.set_title('Probabilities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a probability map using the target points\n",
    "probs_temp = []\n",
    "count = 0\n",
    "\n",
    "for mask in target_mask[:, 2]:\n",
    "    if mask:\n",
    "        probs_temp.append(target_probs['prob'][count])\n",
    "        count += 1\n",
    "    else:\n",
    "        probs_temp.append(np.nan)\n",
    "\n",
    "probs_2d = np.reshape(probs_temp, (num_y, num_x))\n",
    "# probs_2d_ud = np.flipud(np.reshape(probs_temp, (num_y, num_x)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "laterites_frame.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=50)\n",
    "lachlan_bndy.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "\n",
    "plt.imshow(probs_2d, cmap='Spectral_r', origin='lower', interpolation='bicubic', extent=extent_target)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "ax.set_title(f'Prospectivity Map of Co-Ni Laterites')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395b7ad",
   "metadata": {},
   "source": [
    "#### GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_2d_ud = np.flipud(probs_2d)\n",
    "\n",
    "# export the map to a GeoTIFF file\n",
    "xmin, ymin, xmax, ymax = [min(range_x), min(range_y), max(range_x), max(range_y)]\n",
    "geotransform = (xmin, 0.05, 0, ymax, 0, -0.05)\n",
    "map_file = './Datasets/Outputs/probability_map_sup.tif'\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "dataset = driver.Create(map_file, num_x, num_y, 1, gdal.GDT_Float32)\n",
    "dataset.SetGeoTransform(geotransform)\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(4283)\n",
    "dataset.SetProjection(srs.ExportToWkt())\n",
    "dataset.GetRasterBand(1).WriteArray(probs_2d_ud)\n",
    "dataset.FlushCache()\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24086a3c",
   "metadata": {},
   "source": [
    "#### P-A Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the geotiff file (mineral prospectivity map) using rioxarray\n",
    "prospectivity_data = rxr.open_rasterio('./Datasets/Outputs/probability_map_sup.tif').squeeze()\n",
    "\n",
    "# Load the shapefile (mineral occurrences)\n",
    "mineral_occurrences = gpd.read_file('./Datasets/Mineral Occurrences_July22/GSNSWDataset/ni_co_laterites_lachlan.shp')\n",
    "\n",
    "# Extract probability values at known mineral occurrence locations\n",
    "occurrence_probabilities = []\n",
    "for geom in mineral_occurrences.geometry:\n",
    "    x, y = geom.x, geom.y\n",
    "    # Extract probability value at this point\n",
    "    prob_value = prospectivity_data.sel(x=x, y=y, method='nearest').item()\n",
    "    occurrence_probabilities.append(prob_value)\n",
    "\n",
    "# Convert to numpy array for easier manipulation\n",
    "occurrence_probabilities = np.array(occurrence_probabilities)\n",
    "occurrence_probabilities = occurrence_probabilities[~np.isnan(occurrence_probabilities)]\n",
    "\n",
    "# Calculate total study area and the number of cells\n",
    "total_area = np.count_nonzero(~np.isnan(prospectivity_data))\n",
    "total_occurrences = len(occurrence_probabilities)\n",
    "\n",
    "# Define the probability thresholds\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "# Initialize lists to store results\n",
    "occurrence_percentages = []\n",
    "area_percentages = []\n",
    "\n",
    "# Calculate percentages for each threshold\n",
    "for threshold in thresholds:\n",
    "    occurrence_percentage = np.sum(occurrence_probabilities >= threshold) / total_occurrences * 100\n",
    "    area_percentage = np.sum(prospectivity_data >= threshold) / total_area * 100\n",
    "    occurrence_percentages.append(occurrence_percentage)\n",
    "    area_percentages.append(area_percentage)\n",
    "\n",
    "# Convert lists to numpy arrays for plotting\n",
    "occurrence_percentages = np.array(occurrence_percentages)\n",
    "area_percentages = np.array(area_percentages)\n",
    "\n",
    "# Create the P-A plot\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the occurrence percentages\n",
    "ax1.plot(thresholds, occurrence_percentages, 'b-')\n",
    "ax1.set_xlabel('Probability')\n",
    "ax1.set_ylabel('Percentage of Known Mineral Occurrences', color='b')\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.grid(True)  # Add grid to the first y-axis\n",
    "\n",
    "# Create a second y-axis to plot the area percentages\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(thresholds, area_percentages, 'r-')\n",
    "ax2.set_ylabel('Percentage of Study Area', color='r')\n",
    "ax2.set_ylim(100, 0)  # Inverted y-axis\n",
    "\n",
    "# Add horizontal and vertical lines\n",
    "intersection_x = 0.345\n",
    "intersection_y1 = 88\n",
    "intersection_y2 = 12\n",
    "h_line = ax1.hlines(y=intersection_y1, xmin=0, xmax=1, color='g', linestyles=':')\n",
    "v_line = ax1.vlines(x=intersection_x, ymin=0, ymax=100, color='g', linestyles=':')\n",
    "\n",
    "# Add annotations for the lines\n",
    "# X-axis annotation\n",
    "ax1.annotate(f'{intersection_x}', xy=(intersection_x, -0.06), xycoords='data',\n",
    "             xytext=(0, -20), textcoords='offset points',\n",
    "             ha='center', va='top', color='g',\n",
    "             arrowprops=dict(arrowstyle='->', color='g'))\n",
    "\n",
    "# Left Y-axis (Percentage of Known Mineral Occurrences) annotation\n",
    "ax1.annotate(f'{intersection_y1}%', xy=(0, intersection_y1), xycoords='data',\n",
    "             xytext=(-25, 0), textcoords='offset points',\n",
    "             ha='right', va='center', color='g',\n",
    "             arrowprops=dict(arrowstyle='->', color='g', shrinkA=0, shrinkB=4))\n",
    "\n",
    "# Right Y-axis (Percentage of Study Area) annotation\n",
    "ax2.annotate(f'{intersection_y2}%', xy=(1, intersection_y2), xycoords='data',\n",
    "             xytext=(25, 0), textcoords='offset points',\n",
    "             ha='left', va='center', color='g',\n",
    "             arrowprops=dict(arrowstyle='->', color='g', shrinkA=0, shrinkB=4))\n",
    "\n",
    "# Add a text box to explain the intersection point\n",
    "ax1.annotate(f'At probability {intersection_x}:\\n{intersection_y1}% of occurrences\\n{intersection_y2}% of area',\n",
    "             xy=(0.52, 0.92), xycoords='axes fraction',\n",
    "             ha='center', va='top',\n",
    "             bbox=dict(boxstyle='round', fc='white', ec='gray', alpha=0.8))\n",
    "\n",
    "plt.title('Prediction-Area Plot')\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
    "\n",
    "# Adjust the plot margins to make room for annotations\n",
    "plt.subplots_adjust(left=0.15, right=0.85)\n",
    "\n",
    "plt.savefig(\n",
    "    f'./Datasets/Outputs/pa_sup.png',\n",
    "    bbox_inches='tight',\n",
    "    pad_inches=0.1,\n",
    "    dpi=150\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc605a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gawler",
   "language": "python",
   "name": "gawler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
